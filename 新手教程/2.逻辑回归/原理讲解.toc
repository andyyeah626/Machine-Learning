\contentsline {section}{\numberline {1}逻辑回归识别手写数字教程}{2}{section.1}%
\contentsline {subsection}{\numberline {1.1}代码概览}{2}{subsection.1.1}%
\contentsline {subsubsection}{\numberline {1.1.1}导入库}{2}{subsubsection.1.1.1}%
\contentsline {subsection}{\numberline {1.2}MNIST 数据集}{2}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}数据预处理}{3}{subsection.1.3}%
\contentsline {subsubsection}{\numberline {1.3.1}展平图像}{3}{subsubsection.1.3.1}%
\contentsline {subsubsection}{\numberline {1.3.2}归一化到 [0, 1]}{3}{subsubsection.1.3.2}%
\contentsline {subsection}{\numberline {1.4}逻辑回归模型}{3}{subsection.1.4}%
\contentsline {subsubsection}{\numberline {1.4.1}逻辑回归原理}{4}{subsubsection.1.4.1}%
\contentsline {subsubsection}{\numberline {1.4.2}超参数解释}{4}{subsubsection.1.4.2}%
\contentsline {subsection}{\numberline {1.5}模型评估}{4}{subsection.1.5}%
\contentsline {subsection}{\numberline {1.6}逻辑回归核心数学公式}{5}{subsection.1.6}%
\contentsline {subsubsection}{\numberline {1.6.1}二分类情况}{5}{subsubsection.1.6.1}%
\contentsline {subsubsection}{\numberline {1.6.2}多分类情况（Softmax 函数）}{5}{subsubsection.1.6.2}%
\contentsline {section}{\numberline {2}逻辑回归数学原理深度解析}{5}{section.2}%
\contentsline {subsection}{\numberline {2.1}逻辑回归的目标}{5}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}线性组合：$w^T x + b$}{5}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}偏置项 $b$ 的作用}{6}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}从线性模型到概率模型}{6}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}参数 $w$ 和 $b$ 的学习过程}{6}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}训练数据}{6}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}模型预测}{6}{subsubsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.3}损失函数}{7}{subsubsection.2.5.3}%
\contentsline {subsubsection}{\numberline {2.5.4}参数优化}{7}{subsubsection.2.5.4}%
\contentsline {section}{\numberline {3}梯度下降原理详解}{7}{section.3}%
\contentsline {subsection}{\numberline {3.1}直觉理解}{7}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}梯度下降思路}{7}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}数学解释}{7}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}学习率 $\eta $}{8}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}梯度下降示例}{8}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}逻辑回归中的梯度公式}{8}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}核心概念总结}{9}{subsection.3.7}%
